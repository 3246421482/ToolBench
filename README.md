<div align= "center">
    <h1> üõ†Ô∏èToolBenchü§ñ</h1>
</div>

<div align="center">

![Dialogues](https://img.shields.io/badge/Tool\_Num-29-red?style=flat-square)
![Dialogues](https://img.shields.io/badge/API\_Num-86-red?style=flat-square)
![Dialogues](https://img.shields.io/badge/Current\_Dataset\_Size-98K-red?style=flat-square)
![Dialogues](https://img.shields.io/badge/Total\_API\_Call-312K-red?style=flat-square)
![Dialogues](https://img.shields.io/badge/Tool\_LLaMA-Released-green?style=flat-square)

</div>

<p align="center">
  <a href="#model">Model</a> ‚Ä¢
  <a href="#data">Data Release</a> ‚Ä¢
  <a href="https://github.com/OpenBMB/BMTools">Toolkit</a> ‚Ä¢
  <a href="https://arxiv.org/abs/2304.08354">Paper</a> ‚Ä¢
  <a href="https://github.com/thunlp/ToolLearningPapers">Paper List</a> ‚Ä¢
  <a href="#citation">Citation</a> ‚Ä¢

</p>

</div>


üî®This project aims to construct **open-source, large-scale, high-quality** instruction tuning SFT data to facilitate the construction of powerful LLMs with general **tool-use** capability. We provide the dataset, the corresponding training and evaluation scripts, and a capable model ToolLLaMA fine-tuned on ToolBench.

<div align="center">
<img src="https://cdn.discordapp.com/attachments/941582479117127680/1111543600879259749/20230526075532.png" width="400px">
</div>

‚ú®‚ú®Features:
 - Both **single-tool** and **multi-tool** scenarios are supported in ToolBench. 
 - ToolBench provides responses that not only include the final answer but also incorporate the model's **chain-of-thought process, tool execution, and tool execution results**. 
 - ToolBench embraces the complexity of real-world scenarios, enabling **multi-step** tool invocations.
 - Another notable advantage is the **diversity** of our API, which is designed for **real-world scenarios** such as weather information, search functionality, stock updates, and PowerPoint automation. 
 - All the data is automatically generated by OpenAI API and filtered by us, the whole data creation process is easy to scale up.

<br>
<div align="center">
<img src="https://cdn.discordapp.com/attachments/941582479117127680/1111210433307750451/ToolLLaMA.png" width="800px">
</div>
<br>

*Please note that current released data is still not the final version. We are conducting extensive post-processing to improve the data quality and increase the coverage of real-world tools.*

üíÅ‚Äç‚ôÇÔ∏èüíÅüíÅ‚Äç‚ôÄÔ∏è**We need your help!** Curating large-scale real-world APIs and their corresponding tool-use SFT data is not easy, we sincerely invite you to join us in building and refining ToolBench. We will list all participants as co-authors in the final paper. Please contact and join [us](mailto:yujiaqin16@gmail.com) if you're interested.

## üóíÔ∏èData

üëêToolBench is intended solely for research and educational purposes and should not be construed as reflecting the opinions or views of the creators, owners, or contributors of this dataset. It is distributed under [CC BY NC 4.0 License](https://creativecommons.org/licenses/by-nc/4.0/).

ToolBench contains both single-tool and multi-tool scenarios, below is the statistics for the single-tool scenario:

| Tool           | Query Num | Chains Num | Chains/Query |
|----------------|-----------|------------|------------------|
| Weather        | 9827      | 23740      | 2.4              |
| Chemical       | 8585      | 29916      | 3.5              |
| Translation    | 10267     | 23011      | 2.2              |
| Map            | 7305      | 23325      | 3.2              |
| Stock          | 11805     | 32550      | 2.8              |
| Meta analysis  | 2526      | 15725      | 6.2              |
| Bing search    | 31089     | 102088     | 3.3              |
| Wolfram        | 16130     | 56169      | 3.5              |
| Database       | 1264      | 6347       | 5                |


### Data Release

Please download our dataset using the following link: [Data](https://drive.google.com/drive/folders/1OaB-hM7eRiWi3TeqHij24VT9MAqgvC0H?usp=drive_link).

### Data Format
Each line in the downloaded data file is a json dict containing the prompt templated for data creation, human instruction (query) for tool use, intermediate thoughts / tool executions loops, and the final answer. Below we show an example for single tool data generation.

```
Tool Descrition:
BMTools Tool_name: translation
Tool action: get_translation
action_input: {"text": target texts, "tgt_lang": target language}

Generated Data:
{
    "prompt": "Answer the following questions as best you can. Specifically, you have access to the following APIs:\n\nget_translation: . Your input should be a json (args json schema): {{\"text\" : string, \"tgt_lang\" : string, }} The Action to trigger this API should be get_translation and the input parameters should be a json dict string. Pay attention to the type of parameters.\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [get_translation]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times, max 7 times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin! Remember: (1) Follow the format, i.e,\nThought:\nAction:\nAction Input:\nObservation:\nFinal Answer:\n (2) Provide as much as useful information in your Final Answer. (3) Do not make up anything, and if your Observation has no link, DO NOT hallucihate one. (4) If you have enough information and want to stop the process, please use \nThought: I have got enough information\nFinal Answer: **your response. \n The Action: MUST be one of the following:get_translation\nQuestion: {input}\n Agent scratchpad (history actions):\n {agent_scratchpad}",
    "query": "My intention is to convert the data provided in ŸÖÿß ŸáŸä ÿßŸÑÿ£ŸÇÿ≥ÿßŸÖ ÿßŸÑÿ´ŸÑÿßÿ´ÿ© ŸÑŸÑŸÇŸàÿßÿ™ ÿßŸÑŸÖÿ≥ŸÑÿ≠ÿ©ÿü into Arabic(ara).\n",
    "chains": [
        {
            "thought": "I need to use the get_translation API to convert the text into Arabic.",
            "action": "get_translation",
            "action_input": "{\"text\": \"What are the three branches of the military?\", \"tgt_lang\": \"ara\"}",
            "observation": "\"ŸÖÿß ŸáŸä ÿßŸÑŸÅÿ±Ÿàÿπ ÿßŸÑÿ´ŸÑÿßÿ´ÿ© ŸÑŸÑÿ¨Ÿäÿ¥ ÿü\""
        }
    ],
    "answer": "The translation of \"What are the three branches of the military?\" into Arabic is \"ŸÖÿß ŸáŸä ÿßŸÑŸÅÿ±Ÿàÿπ ÿßŸÑÿ´ŸÑÿßÿ´ÿ© ŸÑŸÑÿ¨Ÿäÿ¥ ÿü\"."
}

```


Here is an example of the data creation process using BMTools:

<div align="center">

<img src="assets/meta0423.gif" width="700px">

</div>

## ü§ñModel

We release the delta weights of [ToolLLaMA](https://drive.google.com/drive/folders/1OaB-hM7eRiWi3TeqHij24VT9MAqgvC0H) 7b model trained on single-tool dataset (the multi-tool model is on the way). The model is trained on all the single-tool data in a multi-task fashion. Run the following conversion command to get ToolLLaMA weights. Replace `/path/to/*` with the real paths.
```bash
python toolbench/model/apply_delta.py \
    --base-model-path /path/to/llama-7b \
    --target-model-path /path/to/output/ToolLLaMA-7b/weights \
    --delta-path /path/to/delta/weights
```

## üöÄFine-tuning
### Install
Clone this repository and navigate to the ToolLLaMA folder.
```bash
git clone git@github.com:thuqinyj16/ToolLLaMA.git
cd ToolLLaMA
```
Install Package (python>=3.9)
```bash
pip install -r requirements.txt
```

### Data Preprocess
Download our newly released tool data and put them under `data/original/`. For single tool data preprocessing, you can use the following command to process the data for fine-tuning.:
```bash
python data/preprocess.py \
    --tool_mode single
    --tool_data_path data/original/weather_10k.json \
    --output_path data/processed/weather_processed.json
```
For multi tools data preprocessing, you can use:
```bash
python data/preprocess.py \
    --tool_mode multi
    --tool_data_path data/original/meta_file.json \
    --output_path data/processed/meta_file_processed.json
```

### Train
Our code is based on [FastChat](https://github.com/lm-sys/FastChat). You can use the following command to train ToolLLaMA-7b with 4 x A100 (40GB):
```bash
export PYTHONPATH=./
torchrun --nproc_per_node=4 --master_port=20001 toolbench/train/train_mem.py \
    --model_name_or_path huggyllama/llama-7b  \
    --data_path  data/processed/weather_processed.json \
    --bf16 True \
    --output_dir output \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --evaluation_strategy "steps" \
    --eval_steps 1500 \
    --save_strategy "steps" \
    --save_steps 1500 \
    --save_total_limit 8 \
    --learning_rate 5e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.04 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --fsdp "full_shard auto_wrap" \
    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --lazy_preprocess True
```

## Inference

### Install BMTools
The tool execution is supported by [BMTools](https://github.com/OpenBMB/BMTools). First clone BMTools under current directory and build up settings:
```bash
git clone git@github.com:OpenBMB/BMTools.git
cd BMTools
pip install --upgrade pip
pip install -r requirements.txt
python setup.py develop
cd ..
```

Then add your api keys to secret_keys.sh, and start the local tools:
```bash
source BMTools/secret_keys.sh
python BMTools/host_local_tools.py
```

### Inference with Command Line Interface
Prepare for the api keys and python path:
```bash
source BMTools/secret_keys.sh
export PYTHONPATH=BMTools
```

The command below requires around 14GB of GPU memory for ToolLLaMA-7B. Replace `/path/to/ToolLLaMA/weights` with your converted ToolLLaMA weights path.
- For single tool inference:
```bash
python toolbench/inference/inference_single_tool.py \
    --tool_name weather \
    --model_path /path/to/ToolLLaMA/weights
```

- For multi tools inference:
```bash
python toolbench/inference/inference_multi_tools.py \
    --model_path /path/to/ToolLLaMA/weights
```


## Evaluation

The general idea of ToolBench is to train a LLM in our supervised data which then will support in [BMTools](https://github.com/OpenBMB/BMTools).
Each sector of ToolBench has its own challenges and requires particular strategy designs. 

### Model Experiment
- Machine Evaluation 
We randomly sample 100 chain steps in each tool to build our machine evaluation testbed. On average, there are 27 final steps and 73 intermediate tool calling steps. We evaluate the final steps with Rouge-L and the intermediate steps with ExactMatch.

| model_name                   | Downsampling | Beam size | Overall - Final Answer | Overall - Action | Overall - Input |
|------------------------------|--------------|-----------|------------------------|------------------|-----------------|
| cpmbee-finetuned             | 0.05         | 1         | **0.55**               | 0.64             | 0.40            |
| llama7b-finetuned            | 0.05         | 1         | 0.27                   | **0.77**         | 0.53            |
| vicuna7b-finetuned           | 0.05         | 1         | 0.42                   | 0.53             | 0.40            |
| llama7b-finetuned            | 0.5          | 1         | 0.35                   | 0.67             | 0.50            |
| llama7b-finetuned            | 0.7          | 1         | 0.29                   | 0.74             | **0.56**        |

- Human Evaluation
We randomly sample 10 query in each of the following tools: Weather, Map, Stock, Translation, Chemical and WolframAlpha. We evaluate the pass rate of tool calling process, final answer, and the final answer comparison with chatgpt.

| model_name                   | Downsampling | Beam size |  Tool Calling Process  |   Final Answer   |   Comparison   |
|------------------------------|--------------|-----------|------------------------|------------------|----------------|
| llama7b-finetuned            | 0.05         | 1         | **90%**                | **76.7%**        | 11.7%/60%/28.3%|


- ChatGPT Evaluation

We perform an automatic evluation by ChatGPT which scoring answers and tool-use chains from LLaMA and ChatGPT. 

To run the ChatGPT evaluation code:
```bash
python toolbench/evaluation/evaluate_by_chatgpt.py
```

The evaluation prompt for ChatGPT is designed as follows:
```
You are a fair AI assistant for checking the quality of the answers of other two AI assistants. 

    [Question] 

    {data['query']}

    [The Start of Assistant 1's Answer]

    llama chains: {data['llama_chains']}
    llama answer: {data['llama_answer']}

    [The End of Assistant 1's Answer]

    [The Start of Assistant 2's Answer]

    chatgpt chains: {data['chatgpt_chains']}
    chatgpt answer: {data['chatgpt_answer']}

    [The End of Assistant 2's Answer] 

    We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. 
    Please first judge if the answer is correct based on the question, if an assistant gives a wrong answer, the score should be low.
    Please rate the quality, correctness, helpfulness of their responses based on the question.
    Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance, your scores should be supported by reasonable reasons. 
    Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. 
    The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias, and the order in which the responses were presented does not affect your judgement.
    If the two assistants perform equally well, please output the same score for both of them.
```

The evaluation results for 15 cases for 6 tools are as below (higher is better), our ToolLLaMA matches or outperforms ChatGPT in different scenarios.

| Tool                            | ToolLLaMA Score         | ChatGPT Score         |
| ------------------------------- | ------------------- | --------------------- |
| baidu-translation               | 8.0                 | 8.0                   |
| chemical-prop                   | 7.93                | 7.53                  |
| bing-map                        | 7.93                | 7.64                  |
| stock                           | 4.87                | 4.4                   |
| weather                         | 7.20                | 7.47                  |
| wolframalpha                    | 7.67                | 7.80                  |

## TODO
- [ ] Release the rest part of the data for other tools in BMTools.
- [ ] ToolLLaMA will reach GPT-4's tool-use capability.
- [ ] There will be a Chinese version of ToolBench.
- [ ] Support Chinese LLMs, e.g., CPM-bee.

## Citation
Feel free to cite us if you like ToolBench.

```bibtex
@misc{qin2023tool,
      title={Tool Learning with Foundation Models}, 
      author={Yujia Qin and Shengding Hu and Yankai Lin and Weize Chen and Ning Ding and Ganqu Cui and Zheni Zeng and Yufei Huang and Chaojun Xiao and Chi Han and Yi Ren Fung and Yusheng Su and Huadong Wang and Cheng Qian and Runchu Tian and Kunlun Zhu and Shihao Liang and Xingyu Shen and Bokai Xu and Zhen Zhang and Yining Ye and Bowen Li and Ziwei Tang and Jing Yi and Yuzhang Zhu and Zhenning Dai and Lan Yan and Xin Cong and Yaxi Lu and Weilin Zhao and Yuxiang Huang and Junxi Yan and Xu Han and Xian Sun and Dahai Li and Jason Phang and Cheng Yang and Tongshuang Wu and Heng Ji and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2304.08354},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
